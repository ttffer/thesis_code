{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import torch.onnx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_sz=11\n",
    "layer0_output_size =64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"../../data\",    \n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.Resize(input_img_sz), torchvision.transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"../../data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.Resize(input_img_sz), torchvision.transforms.ToTensor()]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (liner0): Linear(in_features=121, out_features=64, bias=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (liner1): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 11, 11])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.liner0 = nn.Linear(input_img_sz*input_img_sz, layer0_output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.liner1 =nn.Linear(layer0_output_size, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.liner0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.liner1(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_name =\"fashion_MNIST\"+str(input_img_sz)+\"x\"+str(input_img_sz)+\"_\"+str(layer0_output_size)+\"_10.pth\"\n",
    "if not (os.path.isfile(file_name)):\n",
    "    epochs = 50\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(test_dataloader, model, loss_fn)\n",
    "        #torch.save(model, file_name)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    #model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    model = torch.load( file_name)\n",
    "    print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 3717.556224 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('liner0.weight',\n",
       "              tensor([[ -94., -202., -242.,  ...,   55.,   51.,  -35.],\n",
       "                      [-111., -132., -126.,  ...,   53.,   24.,    7.],\n",
       "                      [-176.,  -75.,   86.,  ...,  -68.,  -38.,   36.],\n",
       "                      ...,\n",
       "                      [ 109.,  -32., -137.,  ...,  -10.,    2.,  -47.],\n",
       "                      [ -41.,  -85.,   35.,  ...,   43.,  -31.,  -15.],\n",
       "                      [  -8.,   11.,    1.,  ...,   -6.,   -1.,   10.]], device='cuda:0')),\n",
       "             ('liner0.bias',\n",
       "              tensor([ 112.,  -49.,  -11.,  -46.,  -61.,   10.,   62.,   12.,   40.,   -5.,\n",
       "                       -78.,   -9.,  -43.,   44.,   62.,   -1.,  -25.,  106.,  -10.,    8.,\n",
       "                        38.,   -5.,   -4.,   96.,    3.,    7.,   21.,   52.,   33.,  -69.,\n",
       "                        27., -122.,    5.,    4.,   78.,  -10.,    0.,   80.,    0.,  -35.,\n",
       "                        49.,   59.,   18.,   73.,   27.,  -17.,  -30.,   52.,   51., -109.,\n",
       "                        48.,    3.,   11.,   47.,   -3.,   25.,   31.,   12.,   44.,   44.,\n",
       "                        12.,    0.,   41.,   -3.], device='cuda:0')),\n",
       "             ('liner1.weight',\n",
       "              tensor([[ -15.,  -33.,  -54.,   34.,  128.,   -8.,   78.,   80.,  -64.,  -13.,\n",
       "                         35.,  -69.,  -40.,  -72., -202.,  -48., -124.,    7.,    7.,   36.,\n",
       "                       -140.,   15.,   17.,  115.,    2.,   33., -143., -106.,   12.,    0.,\n",
       "                         -8.,  -45., -104.,  -46., -203.,    8.,  -16.,   54.,  -39.,    6.,\n",
       "                         28.,  -33.,   -7.,  -74.,   15.,   97.,   83.,   76.,   21.,  -18.,\n",
       "                         50., -320.,   -2.,  -20.,   24.,   39.,   64.,    0.,  -27., -209.,\n",
       "                         15.,    2.,   57.,  -12.],\n",
       "                      [ -34.,  156.,  272., -100.,   62.,   68.,   49., -231.,  -60.,    1.,\n",
       "                       -152.,  107.,   23., -148., -105.,  -34., -175.,   -8.,  -87.,  -73.,\n",
       "                        -36., -259.,   -3., -271.,    8.,    0.,   79.,  -18.,  -16.,   52.,\n",
       "                       -117., -106.,  -14.,   52.,  -39.,   15., -131.,   62.,   89., -240.,\n",
       "                         43.,   85.,   -2., -228.,  -13.,  133.,  -29., -102.,  -65.,  238.,\n",
       "                          0.,   60.,  -39.,  -91.,  -29.,    8.,  -98.,   15.,  179., -191.,\n",
       "                        -62.,   -8.,  -56.,  -12.],\n",
       "                      [-220.,  -25.,   -9.,   48.,  -75.,  -49.,  119.,    4.,   10.,   11.,\n",
       "                         82.,   15.,    7.,  -23., -175.,   19.,  -59.,   -5.,    5.,   21.,\n",
       "                         42.,   18.,   26., -180.,  -14.,   12.,  -77.,   75.,   62.,   -6.,\n",
       "                        -49.,   41.,  -44.,  -15., -108.,   -7.,  -39., -143.,   23.,   12.,\n",
       "                        -10.,   11.,   19.,   41.,   -3.,   32., -149., -167.,   29.,  -26.,\n",
       "                         47., -349.,   14.,   -7.,   10.,  -48.,   20.,  -40.,   78.,  128.,\n",
       "                         19.,   31.,    4.,  -11.],\n",
       "                      [-286.,   84.,  -49.,   65., -121.,   45.,  -95., -108.,  -16.,   20.,\n",
       "                        -89.,   16.,  -37.,   34.,  171.,  -13.,  144.,   30.,   75.,  -85.,\n",
       "                         65.,  -24.,   25., -103.,    0.,   19.,  -23.,  -70.,   37.,   -8.,\n",
       "                         17.,  -80.,   12.,  -53.,   83.,    0.,    5.,   16.,    8.,  -28.,\n",
       "                          6.,  -64.,   17.,  -13.,   36., -158.,  -84.,  -73., -202.,  -37.,\n",
       "                         -9.,   29.,   38.,    0.,   12.,  -85.,  -37.,  145.,  -27., -374.,\n",
       "                          1.,   37.,   50.,    4.],\n",
       "                      [ -82.,   78.,   -9.,   -7.,  -23.,  -55., -250.,   90.,   22.,   25.,\n",
       "                        -79.,   61.,   34.,  -87., -124.,   21.,   39.,   -4.,  -37.,   15.,\n",
       "                        -23.,  -69.,   23.,  -91.,   -8.,  -11.,  -26.,   30.,  -21.,   24.,\n",
       "                        -96.,   28.,    8.,   -1.,  125.,   11.,   31.,  -14.,  -40.,   23.,\n",
       "                         -3.,  -69.,   17.,   81.,    2.,   30.,  -37., -220.,  -27.,    7.,\n",
       "                        -10.,   54.,   18.,  -37.,   24.,  -91.,   14., -253.,  -47.,  124.,\n",
       "                         12.,   18.,  -44.,    1.],\n",
       "                      [  71.,  180.,  100.,  -80.,   -3.,   54., -122.,   28.,  159.,   -3.,\n",
       "                          4.,  162.,  -89.,   57., -232.,   -8.,  117.,   73.,  -93.,   18.,\n",
       "                         29.,  -31., -155.,   70.,  -14.,  -47.,   32.,   44., -246., -148.,\n",
       "                         60., -109.,  -24.,  -23.,  232.,   -2.,  -18., -177.,  -37.,  -25.,\n",
       "                         46.,   59.,  -61.,   25., -167.,  -32.,   46.,  121.,   -1., -126.,\n",
       "                        157.,  -91., -225.,   38., -259.,   28.,   95.,    2.,   53.,  415.,\n",
       "                       -143., -487.,  228.,  -14.],\n",
       "                      [-171., -128.,    7.,  -19.,  -17.,  -52.,  -64., -128.,   59.,   10.,\n",
       "                       -162., -138.,   19.,  -25., -107.,   37.,  -59.,    5.,   32.,   32.,\n",
       "                        -69.,   35.,    5.,  112.,    7.,   12.,   73.,  -20.,  -18.,    5.,\n",
       "                         13.,   49.,   30.,    2., -132.,   14.,  -10.,   -5.,  -41.,   31.,\n",
       "                          3.,   -4.,   23., -179.,   19.,  -31.,   51.,  124.,   41.,  -70.,\n",
       "                        -41.,  250.,   14.,   26.,   44.,  -81.,   20.,   -9.,  -65.,  -92.,\n",
       "                         10.,  -14.,   18.,   -8.],\n",
       "                      [  47.,  -89.,  -25.,  -41.,   34., -151.,   94.,   33., -246., -114.,\n",
       "                         54., -451.,  -73.,    7.,  117.,  -13., -100.,   89.,  -78.,  -52.,\n",
       "                         -7.,   40.,  -56.,   27.,   10., -161.,   46.,    0.,   40.,  -29.,\n",
       "                         32.,   38.,   25.,   33.,   75.,   10.,   18.,   90.,    6.,    7.,\n",
       "                       -427.,   17., -233.,   21.,   42., -300.,   -4., -158.,    7.,  162.,\n",
       "                       -277.,   88.,   27.,   18.,   38.,    6., -266.,   88.,  108.,  -93.,\n",
       "                        -40., -215., -396.,   10.],\n",
       "                      [  38.,   22.,   65., -110.,   27.,  127.,   45., -206.,  -16.,   30.,\n",
       "                          2.,  -45.,   40.,   40.,  231.,   21.,   49., -193.,   22.,   10.,\n",
       "                         40.,   22.,    7.,  117.,   -8.,   28., -117., -340.,   11.,   45.,\n",
       "                       -120.,   64.,   27.,   19.,   56.,   -9.,   81.,  -38., -130.,   23.,\n",
       "                        -17.,   32.,  -40.,   92.,   28.,  -84.,    5., -267.,   11.,  144.,\n",
       "                         96., -600.,  -49.,  -10.,  -20.,   17.,  -66., -193.,   23., -140.,\n",
       "                         42.,  -23.,  -54.,    8.],\n",
       "                      [ -34.,  -59., -246.,   47.,  -23.,   60.,  133.,   49.,  -41., -145.,\n",
       "                         63.,  -85.,  -34.,   18.,  -90.,    4.,   48., -167.,   25.,  -25.,\n",
       "                         54.,   37.,  -57., -278.,   -1.,  -93.,   42.,  -72.,  -52.,   -4.,\n",
       "                         55.,   -1.,   27.,   47., -302.,    1.,   37.,   15.,   71.,   -9.,\n",
       "                         67.,  -30.,  -45.,   10., -133., -160.,   28., -186.,   32., -103.,\n",
       "                        -69.,   33.,  -24.,   40., -102.,   38.,  -85.,    5., -319., -256.,\n",
       "                       -120.,   40., -192.,  -11.]], device='cuda:0')),\n",
       "             ('liner1.bias',\n",
       "              tensor([-10., -12.,   9.,  38., -30.,  16.,  -8.,  35.,  -4.,   0.],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load( file_name)\n",
    "for ele in model.state_dict():#turn to numpy\n",
    "    model.state_dict()[ele]=model.state_dict()[ele].cpu().numpy()\n",
    "#turn float to int\n",
    "for layers in model.state_dict():\n",
    "    try:\n",
    "        model.state_dict()[layers].shape[1]## if parameter is 2D\n",
    "        row,col=model.state_dict()[layers].shape\n",
    "        for row in range(model.state_dict()[layers].shape[0]):\n",
    "            for col in range(model.state_dict()[layers].shape[1]):\n",
    "                num=int((model.state_dict()[layers][row][col]+1)/2*255-127)\n",
    "                model.state_dict()[layers][row][col]= num\n",
    "    except:\n",
    "        #model[layers].shape #1D\n",
    "        # print(model[layers].shape[0])\n",
    "        for i in range(model.state_dict()[layers].shape[0]):\n",
    "            num=int((model.state_dict()[layers][i]+1)/2*255-127)\n",
    "            model.state_dict()[layers][i]=num\n",
    "test(test_dataloader, model, loss_fn)\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 18235.304799 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('liner0.weight',\n",
       "              tensor([[-118., -177., -236.,  ...,   59.,   59.,  -59.],\n",
       "                      [-118., -118., -118.,  ...,   59.,    0.,    0.],\n",
       "                      [-177.,  -59.,   59.,  ...,  -59.,  -59.,   59.],\n",
       "                      ...,\n",
       "                      [ 118.,  -59., -118.,  ...,    0.,    0.,  -59.],\n",
       "                      [ -59.,  -59.,   59.,  ...,   59.,  -59.,    0.],\n",
       "                      [   0.,    0.,    0.,  ...,    0.,    0.,    0.]], device='cuda:0')),\n",
       "             ('liner0.bias',\n",
       "              tensor([ 118.,  -59.,    0.,  -59.,  -59.,    0.,   59.,    0.,   59.,    0.,\n",
       "                       -59.,    0.,  -59.,   59.,   59.,    0.,    0.,  118.,    0.,    0.,\n",
       "                        59.,    0.,    0.,  118.,    0.,    0.,    0.,   59.,   59.,  -59.,\n",
       "                         0., -118.,    0.,    0.,   59.,    0.,    0.,   59.,    0.,  -59.,\n",
       "                        59.,   59.,    0.,   59.,    0.,    0.,  -59.,   59.,   59., -118.,\n",
       "                        59.,    0.,    0.,   59.,    0.,    0.,   59.,    0.,   59.,   59.,\n",
       "                         0.,    0.,   59.,    0.], device='cuda:0')),\n",
       "             ('liner1.weight',\n",
       "              tensor([[   0.,  -59.,  -59.,   59.,  118.,    0.,   59.,   59.,  -59.,    0.,\n",
       "                         59.,  -59.,  -59.,  -59., -177.,  -59., -118.,    0.,    0.,   59.,\n",
       "                       -118.,    0.,    0.,  118.,    0.,   59., -118., -118.,    0.,    0.,\n",
       "                          0.,  -59., -118.,  -59., -177.,    0.,    0.,   59.,  -59.,    0.,\n",
       "                          0.,  -59.,    0.,  -59.,    0.,  118.,   59.,   59.,    0.,    0.,\n",
       "                         59., -295.,    0.,    0.,    0.,   59.,   59.,    0.,    0., -236.,\n",
       "                          0.,    0.,   59.,    0.],\n",
       "                      [ -59.,  177.,  295., -118.,   59.,   59.,   59., -236.,  -59.,    0.,\n",
       "                       -177.,  118.,    0., -177., -118.,  -59., -177.,    0.,  -59.,  -59.,\n",
       "                        -59., -236.,    0., -295.,    0.,    0.,   59.,    0.,    0.,   59.,\n",
       "                       -118., -118.,    0.,   59.,  -59.,    0., -118.,   59.,  118., -236.,\n",
       "                         59.,   59.,    0., -236.,    0.,  118.,    0., -118.,  -59.,  236.,\n",
       "                          0.,   59.,  -59., -118.,    0.,    0., -118.,    0.,  177., -177.,\n",
       "                        -59.,    0.,  -59.,    0.],\n",
       "                      [-236.,    0.,    0.,   59.,  -59.,  -59.,  118.,    0.,    0.,    0.,\n",
       "                         59.,    0.,    0.,    0., -177.,    0.,  -59.,    0.,    0.,    0.,\n",
       "                         59.,    0.,    0., -177.,    0.,    0.,  -59.,   59.,   59.,    0.,\n",
       "                        -59.,   59.,  -59.,    0., -118.,    0.,  -59., -118.,    0.,    0.,\n",
       "                          0.,    0.,    0.,   59.,    0.,   59., -177., -177.,    0.,    0.,\n",
       "                         59., -354.,    0.,    0.,    0.,  -59.,    0.,  -59.,   59.,  118.,\n",
       "                          0.,   59.,    0.,    0.],\n",
       "                      [-295.,   59.,  -59.,   59., -118.,   59., -118., -118.,    0.,    0.,\n",
       "                       -118.,    0.,  -59.,   59.,  177.,    0.,  118.,   59.,   59.,  -59.,\n",
       "                         59.,    0.,    0., -118.,    0.,    0.,    0.,  -59.,   59.,    0.,\n",
       "                          0.,  -59.,    0.,  -59.,   59.,    0.,    0.,    0.,    0.,    0.,\n",
       "                          0.,  -59.,    0.,    0.,   59., -177.,  -59.,  -59., -177.,  -59.,\n",
       "                          0.,    0.,   59.,    0.,    0.,  -59.,  -59.,  118.,    0., -354.,\n",
       "                          0.,   59.,   59.,    0.],\n",
       "                      [ -59.,   59.,    0.,    0.,    0.,  -59., -236.,  118.,    0.,    0.,\n",
       "                        -59.,   59.,   59.,  -59., -118.,    0.,   59.,    0.,  -59.,    0.,\n",
       "                          0.,  -59.,    0., -118.,    0.,    0.,    0.,   59.,    0.,    0.,\n",
       "                       -118.,    0.,    0.,    0.,  118.,    0.,   59.,    0.,  -59.,    0.,\n",
       "                          0.,  -59.,    0.,   59.,    0.,   59.,  -59., -236.,    0.,    0.,\n",
       "                          0.,   59.,    0.,  -59.,    0., -118.,    0., -236.,  -59.,  118.,\n",
       "                          0.,    0.,  -59.,    0.],\n",
       "                      [  59.,  177.,  118.,  -59.,    0.,   59., -118.,    0.,  177.,    0.,\n",
       "                          0.,  177., -118.,   59., -236.,    0.,  118.,   59., -118.,    0.,\n",
       "                          0.,  -59., -177.,   59.,    0.,  -59.,   59.,   59., -236., -177.,\n",
       "                         59., -118.,    0.,    0.,  236.,    0.,    0., -177.,  -59.,    0.,\n",
       "                         59.,   59.,  -59.,    0., -177.,  -59.,   59.,  118.,    0., -118.,\n",
       "                        177., -118., -236.,   59., -236.,    0.,  118.,    0.,   59.,  413.,\n",
       "                       -118., -472.,  236.,    0.],\n",
       "                      [-177., -118.,    0.,    0.,    0.,  -59.,  -59., -118.,   59.,    0.,\n",
       "                       -177., -118.,    0.,    0., -118.,   59.,  -59.,    0.,   59.,   59.,\n",
       "                        -59.,   59.,    0.,  118.,    0.,    0.,   59.,    0.,    0.,    0.,\n",
       "                          0.,   59.,   59.,    0., -118.,    0.,    0.,    0.,  -59.,   59.,\n",
       "                          0.,    0.,    0., -177.,    0.,  -59.,   59.,  118.,   59.,  -59.,\n",
       "                        -59.,  236.,    0.,    0.,   59.,  -59.,    0.,    0.,  -59., -118.,\n",
       "                          0.,    0.,    0.,    0.],\n",
       "                      [  59., -118.,    0.,  -59.,   59., -177.,  118.,   59., -236., -118.,\n",
       "                         59., -472.,  -59.,    0.,  118.,    0., -118.,  118.,  -59.,  -59.,\n",
       "                          0.,   59.,  -59.,    0.,    0., -177.,   59.,    0.,   59.,    0.,\n",
       "                         59.,   59.,    0.,   59.,   59.,    0.,    0.,  118.,    0.,    0.,\n",
       "                       -413.,    0., -236.,    0.,   59., -295.,    0., -177.,    0.,  177.,\n",
       "                       -295.,   59.,    0.,    0.,   59.,    0., -295.,   59.,  118., -118.,\n",
       "                        -59., -236., -413.,    0.],\n",
       "                      [  59.,    0.,   59., -118.,    0.,  118.,   59., -177.,    0.,   59.,\n",
       "                          0.,  -59.,   59.,   59.,  236.,    0.,   59., -177.,    0.,    0.,\n",
       "                         59.,    0.,    0.,  118.,    0.,    0., -118., -354.,    0.,   59.,\n",
       "                       -118.,   59.,    0.,    0.,   59.,    0.,   59.,  -59., -118.,    0.,\n",
       "                          0.,   59.,  -59.,  118.,    0.,  -59.,    0., -295.,    0.,  118.,\n",
       "                        118., -590.,  -59.,    0.,    0.,    0.,  -59., -177.,    0., -118.,\n",
       "                         59.,    0.,  -59.,    0.],\n",
       "                      [ -59.,  -59., -236.,   59.,    0.,   59.,  118.,   59.,  -59., -118.,\n",
       "                         59.,  -59.,  -59.,    0., -118.,    0.,   59., -177.,    0.,    0.,\n",
       "                         59.,   59.,  -59., -295.,    0., -118.,   59.,  -59.,  -59.,    0.,\n",
       "                         59.,    0.,    0.,   59., -295.,    0.,   59.,    0.,   59.,    0.,\n",
       "                         59.,  -59.,  -59.,    0., -118., -177.,    0., -177.,   59., -118.,\n",
       "                        -59.,   59.,    0.,   59., -118.,   59.,  -59.,    0., -295., -236.,\n",
       "                       -118.,   59., -177.,    0.]], device='cuda:0')),\n",
       "             ('liner1.bias',\n",
       "              tensor([  0.,   0.,   0.,  59., -59.,   0.,   0.,  59.,   0.,   0.],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load( file_name)\n",
    "A=59\n",
    "for ele in model.state_dict():#turn to numpy\n",
    "    model.state_dict()[ele]=model.state_dict()[ele].cpu().numpy()\n",
    "#turn float to int\n",
    "for layers in model.state_dict():\n",
    "    try:\n",
    "        model.state_dict()[layers].shape[1]## if parameter is 2D\n",
    "        row,col=model.state_dict()[layers].shape\n",
    "        for row in range(model.state_dict()[layers].shape[0]):\n",
    "            for col in range(model.state_dict()[layers].shape[1]):\n",
    "                num=int((model.state_dict()[layers][row][col]+1)/2*255-127)\n",
    "                model.state_dict()[layers][row][col]= round(num/A)*A\n",
    "    except:\n",
    "        #model[layers].shape #1D\n",
    "        # print(model[layers].shape[0])\n",
    "        for i in range(model.state_dict()[layers].shape[0]):\n",
    "            num=int((model.state_dict()[layers][i]+1)/2*255-127)\n",
    "            model.state_dict()[layers][i]=round(num/A)*A\n",
    "test(test_dataloader, model, loss_fn)\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 25612914.031847 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('liner0.weight',\n",
       "              tensor([[ -7802., -16766., -20086.,  ...,   4565.,   4233.,  -2905.],\n",
       "                      [ -9213., -10956., -10458.,  ...,   4399.,   1992.,    581.],\n",
       "                      [-14608.,  -6225.,   7138.,  ...,  -5644.,  -3154.,   2988.],\n",
       "                      ...,\n",
       "                      [  9047.,  -2656., -11371.,  ...,   -830.,    166.,  -3901.],\n",
       "                      [ -3403.,  -7055.,   2905.,  ...,   3569.,  -2573.,  -1245.],\n",
       "                      [  -664.,    913.,     83.,  ...,   -498.,    -83.,    830.]],\n",
       "                     device='cuda:0')),\n",
       "             ('liner0.bias',\n",
       "              tensor([  9296.,  -4067.,   -913.,  -3818.,  -5063.,    830.,   5146.,    996.,\n",
       "                        3320.,   -415.,  -6474.,   -747.,  -3569.,   3652.,   5146.,    -83.,\n",
       "                       -2075.,   8798.,   -830.,    664.,   3154.,   -415.,   -332.,   7968.,\n",
       "                         249.,    581.,   1743.,   4316.,   2739.,  -5727.,   2241., -10126.,\n",
       "                         415.,    332.,   6474.,   -830.,      0.,   6640.,      0.,  -2905.,\n",
       "                        4067.,   4897.,   1494.,   6059.,   2241.,  -1411.,  -2490.,   4316.,\n",
       "                        4233.,  -9047.,   3984.,    249.,    913.,   3901.,   -249.,   2075.,\n",
       "                        2573.,    996.,   3652.,   3652.,    996.,      0.,   3403.,   -249.],\n",
       "                     device='cuda:0')),\n",
       "             ('liner1.weight',\n",
       "              tensor([[ -1245.,  -2739.,  -4482.,   2822.,  10624.,   -664.,   6474.,   6640.,\n",
       "                        -5312.,  -1079.,   2905.,  -5727.,  -3320.,  -5976., -16766.,  -3984.,\n",
       "                       -10292.,    581.,    581.,   2988., -11620.,   1245.,   1411.,   9545.,\n",
       "                          166.,   2739., -11869.,  -8798.,    996.,      0.,   -664.,  -3735.,\n",
       "                        -8632.,  -3818., -16849.,    664.,  -1328.,   4482.,  -3237.,    498.,\n",
       "                         2324.,  -2739.,   -581.,  -6142.,   1245.,   8051.,   6889.,   6308.,\n",
       "                         1743.,  -1494.,   4150., -26560.,   -166.,  -1660.,   1992.,   3237.,\n",
       "                         5312.,      0.,  -2241., -17347.,   1245.,    166.,   4731.,   -996.],\n",
       "                      [ -2822.,  12948.,  22576.,  -8300.,   5146.,   5644.,   4067., -19173.,\n",
       "                        -4980.,     83., -12616.,   8881.,   1909., -12284.,  -8715.,  -2822.,\n",
       "                       -14525.,   -664.,  -7221.,  -6059.,  -2988., -21497.,   -249., -22493.,\n",
       "                          664.,      0.,   6557.,  -1494.,  -1328.,   4316.,  -9711.,  -8798.,\n",
       "                        -1162.,   4316.,  -3237.,   1245., -10873.,   5146.,   7387., -19920.,\n",
       "                         3569.,   7055.,   -166., -18924.,  -1079.,  11039.,  -2407.,  -8466.,\n",
       "                        -5395.,  19754.,      0.,   4980.,  -3237.,  -7553.,  -2407.,    664.,\n",
       "                        -8134.,   1245.,  14857., -15853.,  -5146.,   -664.,  -4648.,   -996.],\n",
       "                      [-18260.,  -2075.,   -747.,   3984.,  -6225.,  -4067.,   9877.,    332.,\n",
       "                          830.,    913.,   6806.,   1245.,    581.,  -1909., -14525.,   1577.,\n",
       "                        -4897.,   -415.,    415.,   1743.,   3486.,   1494.,   2158., -14940.,\n",
       "                        -1162.,    996.,  -6391.,   6225.,   5146.,   -498.,  -4067.,   3403.,\n",
       "                        -3652.,  -1245.,  -8964.,   -581.,  -3237., -11869.,   1909.,    996.,\n",
       "                         -830.,    913.,   1577.,   3403.,   -249.,   2656., -12367., -13861.,\n",
       "                         2407.,  -2158.,   3901., -28967.,   1162.,   -581.,    830.,  -3984.,\n",
       "                         1660.,  -3320.,   6474.,  10624.,   1577.,   2573.,    332.,   -913.],\n",
       "                      [-23738.,   6972.,  -4067.,   5395., -10043.,   3735.,  -7885.,  -8964.,\n",
       "                        -1328.,   1660.,  -7387.,   1328.,  -3071.,   2822.,  14193.,  -1079.,\n",
       "                        11952.,   2490.,   6225.,  -7055.,   5395.,  -1992.,   2075.,  -8549.,\n",
       "                            0.,   1577.,  -1909.,  -5810.,   3071.,   -664.,   1411.,  -6640.,\n",
       "                          996.,  -4399.,   6889.,      0.,    415.,   1328.,    664.,  -2324.,\n",
       "                          498.,  -5312.,   1411.,  -1079.,   2988., -13114.,  -6972.,  -6059.,\n",
       "                       -16766.,  -3071.,   -747.,   2407.,   3154.,      0.,    996.,  -7055.,\n",
       "                        -3071.,  12035.,  -2241., -31042.,     83.,   3071.,   4150.,    332.],\n",
       "                      [ -6806.,   6474.,   -747.,   -581.,  -1909.,  -4565., -20750.,   7470.,\n",
       "                         1826.,   2075.,  -6557.,   5063.,   2822.,  -7221., -10292.,   1743.,\n",
       "                         3237.,   -332.,  -3071.,   1245.,  -1909.,  -5727.,   1909.,  -7553.,\n",
       "                         -664.,   -913.,  -2158.,   2490.,  -1743.,   1992.,  -7968.,   2324.,\n",
       "                          664.,    -83.,  10375.,    913.,   2573.,  -1162.,  -3320.,   1909.,\n",
       "                         -249.,  -5727.,   1411.,   6723.,    166.,   2490.,  -3071., -18260.,\n",
       "                        -2241.,    581.,   -830.,   4482.,   1494.,  -3071.,   1992.,  -7553.,\n",
       "                         1162., -20999.,  -3901.,  10292.,    996.,   1494.,  -3652.,     83.],\n",
       "                      [  5893.,  14940.,   8300.,  -6640.,   -249.,   4482., -10126.,   2324.,\n",
       "                        13197.,   -249.,    332.,  13446.,  -7387.,   4731., -19256.,   -664.,\n",
       "                         9711.,   6059.,  -7719.,   1494.,   2407.,  -2573., -12865.,   5810.,\n",
       "                        -1162.,  -3901.,   2656.,   3652., -20418., -12284.,   4980.,  -9047.,\n",
       "                        -1992.,  -1909.,  19256.,   -166.,  -1494., -14691.,  -3071.,  -2075.,\n",
       "                         3818.,   4897.,  -5063.,   2075., -13861.,  -2656.,   3818.,  10043.,\n",
       "                          -83., -10458.,  13031.,  -7553., -18675.,   3154., -21497.,   2324.,\n",
       "                         7885.,    166.,   4399.,  34445., -11869., -40421.,  18924.,  -1162.],\n",
       "                      [-14193., -10624.,    581.,  -1577.,  -1411.,  -4316.,  -5312., -10624.,\n",
       "                         4897.,    830., -13446., -11454.,   1577.,  -2075.,  -8881.,   3071.,\n",
       "                        -4897.,    415.,   2656.,   2656.,  -5727.,   2905.,    415.,   9296.,\n",
       "                          581.,    996.,   6059.,  -1660.,  -1494.,    415.,   1079.,   4067.,\n",
       "                         2490.,    166., -10956.,   1162.,   -830.,   -415.,  -3403.,   2573.,\n",
       "                          249.,   -332.,   1909., -14857.,   1577.,  -2573.,   4233.,  10292.,\n",
       "                         3403.,  -5810.,  -3403.,  20750.,   1162.,   2158.,   3652.,  -6723.,\n",
       "                         1660.,   -747.,  -5395.,  -7636.,    830.,  -1162.,   1494.,   -664.],\n",
       "                      [  3901.,  -7387.,  -2075.,  -3403.,   2822., -12533.,   7802.,   2739.,\n",
       "                       -20418.,  -9462.,   4482., -37433.,  -6059.,    581.,   9711.,  -1079.,\n",
       "                        -8300.,   7387.,  -6474.,  -4316.,   -581.,   3320.,  -4648.,   2241.,\n",
       "                          830., -13363.,   3818.,      0.,   3320.,  -2407.,   2656.,   3154.,\n",
       "                         2075.,   2739.,   6225.,    830.,   1494.,   7470.,    498.,    581.,\n",
       "                       -35441.,   1411., -19339.,   1743.,   3486., -24900.,   -332., -13114.,\n",
       "                          581.,  13446., -22991.,   7304.,   2241.,   1494.,   3154.,    498.,\n",
       "                       -22078.,   7304.,   8964.,  -7719.,  -3320., -17845., -32868.,    830.],\n",
       "                      [  3154.,   1826.,   5395.,  -9130.,   2241.,  10541.,   3735., -17098.,\n",
       "                        -1328.,   2490.,    166.,  -3735.,   3320.,   3320.,  19173.,   1743.,\n",
       "                         4067., -16019.,   1826.,    830.,   3320.,   1826.,    581.,   9711.,\n",
       "                         -664.,   2324.,  -9711., -28220.,    913.,   3735.,  -9960.,   5312.,\n",
       "                         2241.,   1577.,   4648.,   -747.,   6723.,  -3154., -10790.,   1909.,\n",
       "                        -1411.,   2656.,  -3320.,   7636.,   2324.,  -6972.,    415., -22161.,\n",
       "                          913.,  11952.,   7968., -49800.,  -4067.,   -830.,  -1660.,   1411.,\n",
       "                        -5478., -16019.,   1909., -11620.,   3486.,  -1909.,  -4482.,    664.],\n",
       "                      [ -2822.,  -4897., -20418.,   3901.,  -1909.,   4980.,  11039.,   4067.,\n",
       "                        -3403., -12035.,   5229.,  -7055.,  -2822.,   1494.,  -7470.,    332.,\n",
       "                         3984., -13861.,   2075.,  -2075.,   4482.,   3071.,  -4731., -23074.,\n",
       "                          -83.,  -7719.,   3486.,  -5976.,  -4316.,   -332.,   4565.,    -83.,\n",
       "                         2241.,   3901., -25066.,     83.,   3071.,   1245.,   5893.,   -747.,\n",
       "                         5561.,  -2490.,  -3735.,    830., -11039., -13280.,   2324., -15438.,\n",
       "                         2656.,  -8549.,  -5727.,   2739.,  -1992.,   3320.,  -8466.,   3154.,\n",
       "                        -7055.,    415., -26477., -21248.,  -9960.,   3320., -15936.,   -913.]],\n",
       "                     device='cuda:0')),\n",
       "             ('liner1.bias',\n",
       "              tensor([ -830.,  -996.,   747.,  3154., -2490.,  1328.,  -664.,  2905.,  -332.,\n",
       "                          0.], device='cuda:0'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load( file_name)\n",
    "A=83\n",
    "for ele in model.state_dict():#turn to numpy\n",
    "    model.state_dict()[ele]=model.state_dict()[ele].cpu().numpy()\n",
    "#turn float to int\n",
    "for layers in model.state_dict():\n",
    "    try:\n",
    "        model.state_dict()[layers].shape[1]## if parameter is 2D\n",
    "        row,col=model.state_dict()[layers].shape\n",
    "        for row in range(model.state_dict()[layers].shape[0]):\n",
    "            for col in range(model.state_dict()[layers].shape[1]):\n",
    "                num=int((model.state_dict()[layers][row][col]+1)/2*255-127)\n",
    "                model.state_dict()[layers][row][col]= round(num)*A\n",
    "    except:\n",
    "        #model[layers].shape #1D\n",
    "        # print(model[layers].shape[0])\n",
    "        for i in range(model.state_dict()[layers].shape[0]):\n",
    "            num=int((model.state_dict()[layers][i]+1)/2*255-127)\n",
    "            model.state_dict()[layers][i]=round(num)*A\n",
    "test(test_dataloader, model, loss_fn)\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn img from float to int\n",
    "def int_conver(np_a):\n",
    "    for idx,ele in enumerate(np_a):\n",
    "        np_a[idx]=int((ele+1)/2*255-127)\n",
    "\n",
    "    return np_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.25107665e+11 -2.56236694e+11 -1.61269147e+11 -1.52101353e+11\n",
      " -1.07478764e+11 -6.30650736e+10 -8.74490976e+10 -6.55133965e+09\n",
      " -3.60258329e+10  6.40095706e+10]\n",
      "[-3.93841865e+10 -2.38621500e+11  7.99710744e+10 -1.60683621e+11\n",
      "  3.56597555e+10 -6.51971941e+11 -9.22131984e+09 -5.00761938e+11\n",
      " -9.01847122e+10 -3.49289433e+11]\n",
      "[-1.66880591e+10  1.57852093e+11 -1.20453400e+10 -9.63992825e+10\n",
      " -1.91241007e+10 -4.66600773e+11 -7.99785971e+10 -3.17604907e+11\n",
      " -1.00583692e+11 -2.62704086e+11]\n",
      "[-2.68615690e+10  1.13493064e+11 -2.37184129e+10 -4.36532063e+10\n",
      " -3.14717170e+10 -3.10014843e+11 -7.81996162e+10 -2.92691569e+11\n",
      " -1.35924758e+11 -1.47136268e+11]\n",
      "[ 2.74894583e+10 -1.32590490e+11  6.74050146e+09 -4.88962726e+10\n",
      " -3.17533548e+09 -3.90092069e+11  3.53061174e+10 -2.88543282e+11\n",
      " -2.05993228e+10 -1.97623455e+11]\n",
      "[ 1.38926321e+10  1.18399348e+11 -5.57855990e+09 -9.60025795e+10\n",
      "  4.02258794e+09 -4.62319383e+11 -4.79769187e+10 -2.60150096e+11\n",
      " -5.43758556e+10 -2.51020300e+11]\n",
      "[-5.84780282e+10 -4.90568662e+10 -1.88271196e+10 -5.10458748e+10\n",
      "  8.85024501e+10 -2.98719460e+11 -9.53731827e+09 -2.14703625e+11\n",
      " -6.65703431e+10 -1.93480121e+11]\n",
      "[-5.80267643e+10 -1.06256405e+11 -2.80356390e+10 -6.51842453e+10\n",
      "  3.72133422e+10 -3.38513823e+11  4.04169219e+10 -2.73587697e+11\n",
      " -1.02933172e+11 -1.49770125e+11]\n",
      "[-3.63064431e+10 -3.39230559e+10 -3.23228497e+10 -4.01445834e+10\n",
      " -2.72317028e+10  5.41355664e+09 -3.65106053e+10 -3.25377637e+10\n",
      " -1.97951836e+10 -5.42644257e+10]\n",
      "[-7.17648288e+10 -8.30508365e+10 -8.43377895e+10 -1.09167968e+11\n",
      " -6.87629349e+10 -2.33822842e+10 -7.28794619e+10  3.84791737e+10\n",
      "  4.89091112e+08 -6.71774635e+09]\n",
      "10\n",
      "0:  25\n",
      "1:  42\n"
     ]
    }
   ],
   "source": [
    "hit=0\n",
    "lrsb0_len=layer0_output_size\n",
    "lrsb2_len=10\n",
    "max_0,min_0=0,0\n",
    "max_1,min_1=0,0\n",
    "\n",
    "lrsw0=model.state_dict()[\"liner0.weight\"].reshape(-1).cpu().numpy()\n",
    "lrsb0=model.state_dict()[\"liner0.bias\"].reshape(-1).cpu().numpy()\n",
    "lrsw2=model.state_dict()[\"liner1.weight\"].reshape(-1).cpu().numpy()\n",
    "lrsb2=model.state_dict()[\"liner1.bias\"].reshape(-1).cpu().numpy()\n",
    "for r in range(10):    \n",
    "    img,label=test_data[r]\n",
    "    img=img.reshape(-1)\n",
    "    img=int_conver(img)\n",
    "\n",
    "    out1=np.zeros(lrsb0_len)\n",
    "    out2=np.zeros(lrsb2_len)\n",
    "    \n",
    "    for i in range(lrsb0_len):\n",
    "        for j in range(input_img_sz*input_img_sz):\n",
    "            out1[i]+=lrsw0[i*input_img_sz*input_img_sz+j]*img[j]\n",
    "        out1[i]+=lrsb0[i]\n",
    "    # print(out1)\n",
    "\n",
    "    for i in range(lrsb0_len):\n",
    "        if(out1[i]>max_0):\n",
    "            max_0=out1[i]\n",
    "        if(out1[i]<min_0):\n",
    "            min_0=out1[i]\n",
    "        if(out1[i]<0):\n",
    "            out1[i]=0\n",
    "\n",
    "    for i in range(lrsb2_len):\n",
    "        for j in range(lrsb0_len):\n",
    "            out2[i]+=lrsw2[i*lrsb0_len+j]*out1[j]\n",
    "        out2[i]+=lrsb2[i]\n",
    "\n",
    "    for i in range(10):\n",
    "        if(out2[i]>max_1):\n",
    "            max_1=out2[i]\n",
    "        if(out2[i]<min_1):\n",
    "            min_1=out2[i]\n",
    "    print(out2)\n",
    "    if(out2.argmax()==label):\n",
    "        hit+=1\n",
    "\n",
    "    # print(\"now:\",r+1,\"hit:\",hit)\n",
    "print(hit)\n",
    "layer0_bit = len(bin(int(max_0)))\n",
    "layer2_bit = len(bin(int(max_1)))+2\n",
    "print(\"0: \",layer0_bit)\n",
    "print(\"1: \",layer2_bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-(0+(reg<<0)-(reg<<2)+(reg<<5))'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import TCB\n",
    "def D2reg(num,reg_name):\n",
    "    if(num==0):\n",
    "        return None\n",
    "    else:\n",
    "        tcb_str=TCB.Bin2TCB(int(num))\n",
    "        temp=0\n",
    "        if(num>0):\n",
    "            out_s=\"+(0\"\n",
    "        else:\n",
    "            out_s=\"-(0\"\n",
    "        for s in reversed(tcb_str):\n",
    "            if(s==\"+\"):\n",
    "                out_s+=\"+(\"+reg_name+\"<<\"+str(temp)+\")\"\n",
    "            if(s==\"-\"):\n",
    "                out_s+=\"-(\"+reg_name+\"<<\"+str(temp)+\")\"\n",
    "            temp+=1         \n",
    "            \n",
    "        return out_s+\")\"\n",
    "D2reg(-29,\"reg\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wire_cnt 64\n",
      "wire_cnt 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_size=input_img_sz*input_img_sz\n",
    "wire_cnt=layer0_output_size\n",
    "in_buffer_weight_name=\"in_buffer_weight\"\n",
    "weight_to_bias_name=\"weight_bias\"\n",
    "bias_relu_name=\"bias_relu\"\n",
    "in_buffer_weight_parameter=model.state_dict()[\"liner0.weight\"]\n",
    "weight_bias_parameter=model.state_dict()[\"liner0.bias\"]\n",
    "#data_bit_num=20\n",
    "data_bit_num=layer0_bit\n",
    "file_destination = \"verilog_net1\"\n",
    "\n",
    "layer0_verilog_file_name=\"layer0_tcb_\"+str(img_size)+\"x\"+str(layer0_output_size)+\"x10\"\n",
    "f=open(\"./\"+file_destination+\"/\"+layer0_verilog_file_name+\".v\",\"w\")\n",
    "f.write(\"module \"+layer0_verilog_file_name+\"\\n\")\n",
    "f.write(\"(\\n\")\n",
    "\n",
    "f.write(\"\\\n",
    "    input clk,\\n\\\n",
    "    input rst,\\n\\\n",
    "    input [\"+str(img_size)+\"*8-1:0] img,\\n\\\n",
    "    input valid,\\n\\\n",
    "    output  reg ready,\\n\\\n",
    "    output [\"+str(data_bit_num)+\"*\"+str(layer0_output_size)+\"-1:0] layer_out\\n\"\\\n",
    ")\n",
    "f.write(\");\\n\")\n",
    "f.write(\"parameter DATA_WIDTH = \"+str(data_bit_num)+\";\\n\")\n",
    "f.write(\"parameter IMG_SZ   =   \"+str(img_size)+\";\\n\")\n",
    "\n",
    "f.write(\"reg    signed [8-1:0]  in_buffer[0:IMG_SZ-1];\\n\")\n",
    "f.write(\"integer i;\\n\")\n",
    "f.write(\"always@(posedge clk )\\n\\\n",
    "    begin\\n\\\n",
    "        if(rst)\\n\\\n",
    "            begin\\n\\\n",
    "                for(i=0;i<IMG_SZ;i=i+1)\\n\\\n",
    "                    begin\\n\\\n",
    "                        in_buffer[i]<=0;\\n\\\n",
    "                    end\\n\\\n",
    "            end\\n\\\n",
    "        else\\n\\\n",
    "        begin\\n\")\n",
    "\n",
    "for i in range(img_size):\n",
    "    f.write(\"       in_buffer[\"+str(i)+\"]<=img[\"+str(7+i*8)+\":\"+str(0+i*8)+\"];\\n\")           \n",
    "f.write(\"        end\\n   end\\n\")\n",
    "for naming_idx in range(wire_cnt): \n",
    "    f.write(\"wire signed  [DATA_WIDTH-1:0]   \")   \n",
    "    f.write(in_buffer_weight_name+str(naming_idx)+\";\\n\")\n",
    "    f.write(\"assign \"+in_buffer_weight_name+str(naming_idx)+\"=0\")\n",
    "    #insert tcb\n",
    "    for in_buf_idx in range(img_size):\n",
    "        name=\"in_buffer[\" +str(in_buf_idx)+\"]\"\n",
    "        num=int(in_buffer_weight_parameter[naming_idx][in_buf_idx])\n",
    "        try:\n",
    "            f.write(D2reg(num,name))\n",
    "        except:\n",
    "            pass\n",
    "    f.write(\";\\n\")\n",
    "\n",
    "#weight0 to bias0\n",
    "previous_layer_name=in_buffer_weight_name\n",
    "#naming wire\n",
    "print(\"wire_cnt\",wire_cnt)\n",
    "for naming_idx in range(wire_cnt): \n",
    "    f.write(\"wire signed [DATA_WIDTH-1:0]   \")   \n",
    "    f.write(weight_to_bias_name+str(naming_idx)+\";\\n\")\n",
    "for naming_idx in range(wire_cnt):\n",
    "    f.write(\"assign \"+weight_to_bias_name+str(naming_idx)+\"=\")\n",
    "    f.write(previous_layer_name+str(naming_idx))\n",
    "    f.write(\"+(\")\n",
    "    f.write(str(int(weight_bias_parameter[naming_idx])))\n",
    "\n",
    "    f.write(\");\\n\")\n",
    "#bias0 to relu0\n",
    "previous_layer_name=weight_to_bias_name\n",
    "#naming wire\n",
    "print(\"wire_cnt\",wire_cnt)\n",
    "for naming_idx in range(wire_cnt): \n",
    "    f.write(\"wire signed [DATA_WIDTH-1:0]   \")   \n",
    "    f.write(bias_relu_name+str(naming_idx)+\";\\n\")\n",
    "for naming_idx in range(wire_cnt):\n",
    "    f.write(\"assign \"+bias_relu_name+str(naming_idx)+\"=(\")\n",
    "    f.write(previous_layer_name+str(naming_idx))\n",
    "    f.write(\"[DATA_WIDTH-1]==1'b1)   ?   \")\n",
    "    f.write(\"{DATA_WIDTH{1'b0}}:\")\n",
    "    f.write(previous_layer_name+str(naming_idx))\n",
    "    f.write(\";\\n\")\n",
    "\n",
    "\n",
    "f.write(\"assign layer_out={\")\n",
    "for naming_idx in range(wire_cnt-1,-1,-1):\n",
    "    if(naming_idx==0):\n",
    "        f.write(bias_relu_name+str(naming_idx))\n",
    "    else:\n",
    "        f.write(bias_relu_name+str(naming_idx)+\",\")\n",
    "f.write(\"};\\n\")\n",
    "f.write(\"always@(posedge clk)\\n\\\n",
    "    begin\\n\\\n",
    "        if(rst)\\n\\\n",
    "            begin\\n\\\n",
    "                ready<=1'b0;\\n\\\n",
    "            end\\n\\\n",
    "        else\\n\\\n",
    "            begin\\n\\\n",
    "                ready<=valid;\\n\\\n",
    "            end\\n\\\n",
    "    end\\n\")\n",
    "f.write(\"endmodule\")\n",
    "f.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_output_sz=layer0_output_size\n",
    "last_layer_data_sz=layer0_bit\n",
    "\n",
    "wire_cnt=10\n",
    "in_buffer_weight_name=\"in_buffer_weight\"\n",
    "weight_to_bias_name=\"weight_bias\"\n",
    "bias_relu_name=\"bias_relu\"\n",
    "in_buffer_weight_parameter=model.state_dict()[\"liner1.weight\"]\n",
    "weight_bias_parameter=model.state_dict()[\"liner1.bias\"]\n",
    "\n",
    "layer2_verilog_file_name=\"layer2_tcb_\"+str(img_size)+\"x\"+str(layer0_output_size)+\"x10\"\n",
    "f=open(\"./\"+file_destination+\"/\"+layer2_verilog_file_name+\".v\",\"w\")\n",
    "f.write(\"module \"+layer2_verilog_file_name+\"\\n\")\n",
    "f.write(\"(\\n\")\n",
    "\n",
    "f.write(\"\\\n",
    "    input clk,\\n\\\n",
    "    input rst,\\n\")\n",
    "f.write(\"   input valid,\\n\")\n",
    "f.write(\"   output  reg ready,\\n\")\n",
    "f.write(\"    input [\"+str(last_layer_data_sz)+\"*\"+str(last_output_sz)+\"-1:0]  layer_in,\\n\")\n",
    "f.write(\"    output [\"+str(layer2_bit)+\"*10-1:0]   layer_out\\n\\\n",
    ");\\n\")\n",
    "f.write(\"parameter DATA_WIDTH   =   \"+str(layer2_bit)+\";\\n\")\n",
    "f.write(\"reg [DATA_WIDTH-1:0]    layer_in_buffer    [0:\"+str(last_output_sz)+\"-1];\\n\")\n",
    "f.write(\"integer i;\\n\")\n",
    "f.write(\"always@(posedge clk )\\n\\\n",
    "    begin\\n\\\n",
    "        if(rst)\\n\\\n",
    "            begin\\n\")\n",
    "f.write(\"                for(i=0;i<\"+str(last_output_sz)+\";i=i+1)\\n\\\n",
    "                    begin\\n\\\n",
    "                        layer_in_buffer[i]<=0;\\n\\\n",
    "                    end\\n\\\n",
    "            end\\n\\\n",
    "        else\\n\\\n",
    "        begin\\n\")\n",
    "for i in range(last_output_sz):\n",
    "    f.write(\"       layer_in_buffer[\"+str(i)+\"]<=layer_in[\"+str(last_layer_data_sz-1+i*last_layer_data_sz)+\":\"+str(0+i*last_layer_data_sz)+\"];\\n\")           \n",
    "f.write(\"        end\\n   end\\n\")\n",
    "\n",
    "\n",
    "f.write(\"\\n\")\n",
    "previous_layer_name=\"layer_in_buffer\"\n",
    "#naming wire\n",
    "for naming_idx in range(wire_cnt): \n",
    "    f.write(\"wire [DATA_WIDTH-1:0]   \")   \n",
    "    f.write(in_buffer_weight_name+str(naming_idx)+\";\\n\")\n",
    "    f.write(\"assign \"+in_buffer_weight_name+str(naming_idx)+\"=0\")\n",
    "    for previoud_layer_idx in range(last_output_sz):\n",
    "        name=previous_layer_name+\"[\"+str(previoud_layer_idx)+\"]\"\n",
    "        num=int(in_buffer_weight_parameter[naming_idx][previoud_layer_idx])\n",
    "        try:\n",
    "            f.write(D2reg(num,name))\n",
    "        except:\n",
    "            pass\n",
    "    f.write(\";\\n\")\n",
    "# weight4 to bias4 \n",
    "\n",
    "previous_layer_name=in_buffer_weight_name\n",
    "#naming wire\n",
    "for naming_idx in range(wire_cnt): \n",
    "    f.write(\"wire [DATA_WIDTH-1:0]   \")   \n",
    "    f.write(weight_to_bias_name+str(naming_idx)+\";\\n\")\n",
    "    f.write(\"assign \"+weight_to_bias_name+str(naming_idx)+\"=\")\n",
    "    f.write(previous_layer_name+str(naming_idx))\n",
    "    f.write(\"+(\")\n",
    "    f.write(str(int(weight_bias_parameter[naming_idx])))\n",
    "    f.write(\");\\n\")\n",
    "f.write(\"assign layer_out={\\n\")\n",
    "for naming_idx in range(wire_cnt-1,-1,-1): \n",
    "    if(naming_idx==0):\n",
    "        f.write(\"            \"+weight_to_bias_name+str(naming_idx))\n",
    "    else:\n",
    "        f.write(\"            \"+weight_to_bias_name+str(naming_idx)+\",\\n\")\n",
    "f.write(\"};\\n\")\n",
    "f.write(\"always@(posedge clk)\\n\\\n",
    "    begin\\n\\\n",
    "        if(rst)\\n\\\n",
    "            begin\\n\\\n",
    "                ready<=1'b0;\\n\\\n",
    "            end\\n\\\n",
    "        else\\n\\\n",
    "            begin\\n\\\n",
    "                ready<=valid;\\n\\\n",
    "            end\\n\\\n",
    "    end\\n\")\n",
    "f.write(\"endmodule\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sz=layer2_bit\n",
    "f=open(\"./\"+file_destination+\"/comparator_30bit.v\",\"w\")\n",
    "f.write(\"module comparator\\n\")\n",
    "f.write(\"(\\n\")\n",
    "f.write(\"input [\"+str(data_sz)+\"*10-1:0] layer_out,\\n\")\n",
    "f.write(\"input rst,\\n\")\n",
    "f.write(\"input clk,\\n\")\n",
    "f.write(\"input valid,\\n\")\n",
    "f.write(\"output  reg ready,\\n\")\n",
    "f.write(\"output reg [7:0] predict\\n\")\n",
    "f.write(\");\\n\")\n",
    "f.write(\"parameter DATA_WIDTH=\"+str(data_sz)+\";\\n\")\n",
    "f.write(\"reg [DATA_WIDTH-1: 0] result [0:9];\\n\")\n",
    "f.write(\"wire [4+DATA_WIDTH-1:0] com_re01,com_re23,com_re45,com_re67,com_re89;\\n\")\n",
    "f.write(\"reg ready_temp;\\n\")\n",
    "f.write(\"always@(posedge clk)\\n\\\n",
    "    begin\\n\\\n",
    "        if(rst)\\n\\\n",
    "            begin\\n\\\n",
    "                ready<=1'b0;\\n\\\n",
    "                ready_temp<=1'b0;\\n\\\n",
    "            end\\n\\\n",
    "        else\\n\\\n",
    "            begin\\n\\\n",
    "                ready_temp<=valid;\\n\\\n",
    "                ready<=ready_temp;\\n\\\n",
    "            end\\n\\\n",
    "    end\\n\")\n",
    "\n",
    "f.write(\" \\n\\\n",
    "assign com_re01=(result[0][DATA_WIDTH-1]^result[1][DATA_WIDTH-1]) ? \\n\\\n",
    "                                                        ((result[0][DATA_WIDTH-1]==1'b0)   ?   {4'd0,result[0]}:{4'd1,result[1]}):\\n\\\n",
    "                                                        ((result[0]>result[1]) ? {4'd0,result[0]}:{4'd1,result[1]});\\n\\\n",
    "assign com_re23=(result[2][DATA_WIDTH-1]^result[3][DATA_WIDTH-1]) ? \\n\\\n",
    "                                                        ((result[2][DATA_WIDTH-1]==1'b0)   ?   {4'd2,result[2]}:{4'd3,result[3]}):\\n\\\n",
    "                                                        ((result[2]>result[3]) ? {4'd2,result[2]}:{4'd3,result[3]});\\n\\\n",
    "assign com_re45=(result[4][DATA_WIDTH-1]^result[5][DATA_WIDTH-1]) ? \\n\\\n",
    "                                                        ((result[4][DATA_WIDTH-1]==1'b0)   ?   {4'd4,result[4]}:{4'd5,result[5]}):\\n\\\n",
    "                                                        ((result[4]>result[5]) ? {4'd4,result[4]}:{4'd5,result[5]});\\n\")  \n",
    "f.write(\"\\n\\\n",
    "assign com_re67=(result[6][DATA_WIDTH-1]^result[7][DATA_WIDTH-1]) ? \\n\\\n",
    "                                                        ((result[6][DATA_WIDTH-1]==1'b0)   ?   {4'd6,result[6]}:{4'd7,result[7]}):\\n\\\n",
    "                                                        ((result[6]>result[7]) ? {4'd6,result[6]}:{4'd7,result[7]});\\n\\\n",
    "assign com_re89=(result[8][DATA_WIDTH-1]^result[9][DATA_WIDTH-1]) ? \\n\\\n",
    "                                                        ((result[8][DATA_WIDTH-1]==1'b0)   ?   {4'd8,result[8]}:{4'd9,result[9]}):\\n\\\n",
    "                                                        ((result[8]>result[9]) ? {4'd8,result[8]}:{4'd9,result[9]});\\n\\\n",
    "wire [4+DATA_WIDTH-1:0] com_re01_23,com_re45_67,com_re0123_4567,com_re01234567_89;\\n\\\n",
    "assign com_re01_23=(com_re01[DATA_WIDTH-1]^com_re23[DATA_WIDTH-1])   ?\\n\\\n",
    "                                                        ((com_re01[DATA_WIDTH-1]==1'b0)  ?   com_re01:com_re23):\\n\\\n",
    "                                                        ((com_re01[DATA_WIDTH-1:0]>com_re23[DATA_WIDTH-1:0]) ?   com_re01:com_re23);\\n\\\n",
    "assign com_re45_67=(com_re45[DATA_WIDTH-1]^com_re67[DATA_WIDTH-1])   ?\\n\\\n",
    "                                                        ((com_re45[DATA_WIDTH-1]==1'b0)  ?   com_re45:com_re67):\\n\\\n",
    "                                                        ((com_re45[DATA_WIDTH-1:0]>com_re67[DATA_WIDTH-1:0]) ?   com_re45:com_re67);\\n\\\n",
    "assign com_re0123_4567=(com_re01_23[DATA_WIDTH-1]^com_re45_67[DATA_WIDTH-1])   ?\\n\\\n",
    "                                                        ((com_re01_23[DATA_WIDTH-1]==1'b0)  ?   com_re01_23:com_re45_67):\\n\\\n",
    "                                                        ((com_re01_23[DATA_WIDTH-1:0]>com_re45_67[DATA_WIDTH-1:0]) ?   com_re01_23:com_re45_67);\\n\\\n",
    "assign com_re01234567_89=(com_re0123_4567[DATA_WIDTH-1]^com_re89[DATA_WIDTH-1])   ?\\n\\\n",
    "                                                        ((com_re0123_4567[DATA_WIDTH-1]==1'b0)  ?   com_re0123_4567:com_re89):\\n\\\n",
    "                                                        ((com_re0123_4567[DATA_WIDTH-1:0]>com_re89[DATA_WIDTH-1:0]) ?   com_re0123_4567:com_re89);\\n\")\n",
    "f.write(\"integer i;\\n\")\n",
    "f.write(\"always@(posedge clk )\\n\")\n",
    "f.write(\"begin\\n\\\n",
    "    if(rst)\\n\\\n",
    "            begin\\n\\\n",
    "                for(i=0;i<10;i=i+1)\\n\\\n",
    "                    begin\\n\\\n",
    "                        result[i]<={\"+str(data_sz)+\"'b0};\\n\\\n",
    "                    end\\n\\\n",
    "                predict<=0;\\n\\\n",
    "            end\\n\\\n",
    "    else\\n\\\n",
    "    begin \\n\")\n",
    "f.write(\"\\\n",
    "        predict <={4'b0,com_re01234567_89[4+DATA_WIDTH-1:4+DATA_WIDTH-1-3]};\\n\")\n",
    "for i in range(10):\n",
    "    f.write(\"       result[\"+str(i)+\"]<=layer_out[\"+str(data_sz-1+data_sz*i)+\":\"+str(data_sz*i)+\"];\\n\")\n",
    "f.write(\"\\\n",
    "    end\\n\\\n",
    "end\\n\")\n",
    "\n",
    "\n",
    "\n",
    "f.write(\"endmodule\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer0_port=layer0_output_size\n",
    "layer2_port=10\n",
    "tb_name=\"top_tcb_\"+str(input_img_sz*input_img_sz)+\"_\"+str(layer0_output_size)+\"_10\"\n",
    "f=open(\"./\"+file_destination+\"/\"+tb_name+\".v\",\"w\")\n",
    "f.write(\"module \"+tb_name+\"(\\n\")\n",
    "f.write(\"input clk,\\n\\\n",
    "input rst,\\n\\\n",
    "input [\"+str(input_img_sz*input_img_sz)+\"*8-1:0] img_source,\\n\\\n",
    "output [31:0] number,\\n\")\n",
    "f.write(\"input valid_top,\\n\")\n",
    "f.write(\"output  ready_top\\n\")\n",
    "f.write(\");\\n\")\n",
    "f.write(\"wire layer0_ready,layer2_ready\\n;\")\n",
    "f.write(\"wire   [\"+str(layer0_bit)+\"*\"+str(layer0_port)+\"-1:0] layer0_out;\\n\")\n",
    "f.write(\"wire   [\"+str(layer2_bit)+\"*\"+str(layer2_port)+\"-1:0] layer2_out;\\n\")\n",
    "f.write(layer0_verilog_file_name+\" DUT_layer0   (.clk(clk),.rst(rst),.img(img_source),.layer_out(layer0_out),.ready(layer0_ready),.valid(valid_top));\\n\")\n",
    "f.write(layer2_verilog_file_name+\" DUT_layer2   (.clk(clk),.rst(rst),.layer_in(layer0_out),.layer_out(layer2_out),.ready(layer2_ready),.valid(layer0_ready));\\n\")\n",
    "f.write(\"comparator DUT_comparator (.clk(clk),.rst(rst),.layer_out(layer2_out),.predict(number),.ready(ready_top),.valid(layer2_ready));\\n\")\n",
    "\n",
    "f.write(\"endmodule\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f=open(\"./\"+file_destination+\"/tb.v\",\"w\")\n",
    "f.write(\"`timescale 1ns/1ps\\n\")\n",
    "f.write(\"module tb;\\n\")\n",
    "\n",
    "f.write(\"reg clk,rst,valid;\\n\")\n",
    "f.write(\"reg [\"+str(input_img_sz*input_img_sz*8)+\"-1:0] img;\\n\")\n",
    "f.write(\"wire [7:0] number;\\n\")\n",
    "f.write(tb_name+\" top_DUT(\\n\\\n",
    "    .clk(clk),\\n\\\n",
    "    .rst(rst),\\n\\\n",
    "    .img_source(img),\\n\\\n",
    "    .valid_top(valid),\\n\\\n",
    "    .ready_top(ready_top),\\n\\\n",
    "    .number(number)\\n\\\n",
    ");\\n\")\n",
    "f.write(\"always #5 clk=~clk;\\n\")\n",
    "f.write(\"initial \\nbegin\\n\")\n",
    "f.write(\"$monitor(\\\"number is %d\\\",number);\\n\")\n",
    "f.write(\"clk=0;rst=1'b1;valid=1'b1;\\n\")\n",
    "f.write(\"img=\"+str(input_img_sz*input_img_sz)+\"'b0;\\n\")\n",
    "f.write(\"#10 rst=1'b0;\\n\")\n",
    "for i in range(10):\n",
    "    f.write(\"@(negedge clk) #(10/4) img=\"+str(input_img_sz*input_img_sz*8)+\"'b\")\n",
    "    img,idx=test_data[i]\n",
    "    img=img.reshape(-1)\n",
    "    img=np.asarray(img)\n",
    "    img=int_conver(img)\n",
    "    for ele in reversed(img):\n",
    "        eight_bit=\"{:08b}\".format(int(ele))\n",
    "        f.write(eight_bit)\n",
    "        #print(ele)\n",
    "    f.write(\";\\n\")\n",
    "\n",
    "\n",
    "\n",
    "f.write(\"#100 $finish;\\n\")\n",
    "\n",
    "\n",
    "f.write(\"end\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.write(\"endmodule\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"./\"+file_destination+\"/label.txt\",\"w\")\n",
    "for i in range(10):\n",
    "    img,idx=test_data[i]\n",
    "    f.write(str(idx))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
